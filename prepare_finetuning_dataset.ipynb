{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOBP+KjghYW2Mi4/UeCG0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberunity2/Automate_LitReview-/blob/main/prepare_finetuning_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we collect all the necessary input data and prepare the dataset for finetuning. As described the three elements of fine-tuning dataset are:\n",
        "\n",
        "1. The target paper's literature review.\n",
        "2. The target paper's research question.\n",
        "3. Metadata of the target paper's references.\n",
        "4. An instruction prompt.\n"
      ],
      "metadata": {
        "id": "6ABdfnOFO3-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Intro/LitReview from Target Papers (benchmark litreview)**"
      ],
      "metadata": {
        "id": "XFKJFf7SOJ4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzIN984fNx4L"
      },
      "outputs": [],
      "source": [
        "!pip install fitz\n",
        "!pip install --upgrade pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_papers = [file for file in os.listdir('./') if file.endswith('.pdf')]"
      ],
      "metadata": {
        "id": "XV6AOdPCOBXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import re\n",
        "\n",
        "def clean_extracted_text(text):\n",
        "    \"\"\"\n",
        "    Cleans extracted text by removing unwanted characters and formatting issues.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces/newlines with a single space\n",
        "    text = re.sub(r'\\n', ' ', text)  # Remove newline characters\n",
        "    text = re.sub(r'\\b(cid:[0-9]+)\\b', '', text)  # Remove unwanted placeholders like (cid:1234)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove content within brackets []\n",
        "    return text.strip()\n",
        "\n",
        "def remove_unwanted_content(text):\n",
        "    \"\"\"\n",
        "    Removes tables, figures, footnotes, and page numbers from the text using regex patterns.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'Table\\s+\\d+:.*?(\\n|$)', '', text, flags=re.IGNORECASE)  # Remove table captions\n",
        "    text = re.sub(r'Figure\\s+\\d+:.*?(\\n|$)', '', text, flags=re.IGNORECASE)  # Remove figure captions\n",
        "    text = re.sub(r'(Table|Figure)\\s+\\d+.*?(\\n|$)', '', text, flags=re.IGNORECASE)  # Remove generic table/figure mentions\n",
        "    text = re.sub(r'\\d+\\s*-\\s*\\d+', '', text)  # Remove page numbers (e.g., \"1 - 2\")\n",
        "    text = re.sub(r'\\[[^\\]]*\\]', '', text)  # Remove footnotes enclosed in brackets\n",
        "    return text.strip()\n",
        "\n",
        "def extract_text_from_pages(file_path):\n",
        "    \"\"\"\n",
        "    Extracts and processes text from the first 10 pages of a PDF, removing tables, figures, footnotes, and page numbers.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        aggregated_text = \"\"\n",
        "\n",
        "        for page_number in range(1, min(10, len(doc))):  # Process only the first 10 pages (exclude title page)\n",
        "            page = doc[page_number]\n",
        "            page_text = page.get_text(\"text\")\n",
        "            page_text = remove_unwanted_content(page_text)  # Remove tables, figures, footnotes, page numbers\n",
        "            aggregated_text += page_text.strip() + \" \"  # Aggregate text from all pages\n",
        "\n",
        "        doc.close()\n",
        "        return clean_extracted_text(aggregated_text)  # Clean and return the aggregated text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "JN_oZH2SODNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "intros = []\n",
        "for pdf in target_papers[0:1]:\n",
        "    intro_text = extract_text_from_pages(pdf)\n",
        "    intros.append(intro_text)"
      ],
      "metadata": {
        "id": "0VAcybf3OEFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extract Research Question from Target Papers**"
      ],
      "metadata": {
        "id": "UpJv_mZEOOQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we use gpt-4o to extract the research question of each target paper\n",
        "\n",
        "!pip install langchain\n",
        "!pip install --quiet langchain_experimental langchain_openai\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", api_key=\"?\", temperature = 0)\n",
        "\n",
        "research_q_list = []\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "        Do not give information not mentioned in the context information.\n",
        "        If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"], template=template\n",
        ")\n",
        "\n",
        "# Create the chain using LLMChain\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=StrOutputParser())\n",
        "\n",
        "query = \"What is the main research question discussed in the context? Formulate your response in a research question form.\"\n",
        "\n",
        "# Iterate through elements of intros\n",
        "for intro in intros:\n",
        "    context_str = str(intro)\n",
        "    # Use the 'predict' method for multiple input variables\n",
        "    research_q = chain.predict(context=context_str, question=query)\n",
        "    research_q_list.append(research_q)"
      ],
      "metadata": {
        "id": "CUq_Sy-lOYTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metadata for Referenced Papers by Target Paper**"
      ],
      "metadata": {
        "id": "g8JqJ1h3Oa1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we load the metadata collected from Semantic Scholar into a single string for each target paper\n",
        "\n",
        "import pandas as pd\n",
        "target_papers_references = [paper.replace('.pdf', '.csv') for paper in target_papers]\n",
        "metadata_list = []\n",
        "\n",
        "for reference in target_papers_references:\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(reference)\n",
        "\n",
        "    # Replace NaN in 'publicationYear' with '?'\n",
        "    df['publicationYear'] = df['publicationYear'].fillna('?')\n",
        "\n",
        "    # Remove '[' and ']' from 'authors'\n",
        "    df['authors'] = df['authors'].str.replace(r'\\[', '', regex=True).str.replace(r'\\]', '', regex=True)\n",
        "\n",
        "    # Create the metadata string for the current file\n",
        "    metadata_string = '||'.join(\n",
        "        f\"title:'{row['title']}'. abstract:'{row['abstract']}'. authors:'{row['authors']}'. pubyear:'{row['publicationYear']}'\"\n",
        "        for _, row in df.iterrows()\n",
        "    )\n",
        "\n",
        "    # Append the metadata string to the list\n",
        "    metadata_list.append(metadata_string)\n"
      ],
      "metadata": {
        "id": "etZH9Y54OcuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Instruction Prompt and Save the whole Dataset as .json**"
      ],
      "metadata": {
        "id": "7dth8v3yOdXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "data = []\n",
        "\n",
        "# Iterate through your observations to construct the dataset\n",
        "for i in range(len(intros)):\n",
        "    instruction_prompt = f\"\"\"The following is a list of paper metadata separated by ||.\n",
        "     Each element in the list includes: title, abstract, author names, publication year.\n",
        "     The items in this list are the papers referenced by the target paper. list of paper metadata = {metadata_list[i]}.\n",
        "     The following is the research question from the target paper. research question = '{research_q_list[i]}'.\n",
        "     Using abstract of papers content in the list of paper metadata, and considering the research question, learn to write a the target paper's literature review.\n",
        "     Remember target paper's literature review may contain material that are not directly or indirectly related to the content in the list of paper metadata. Ignore those parts in target paper's literature review.\n",
        "     The following is target paper's literature review:\"\"\"\n",
        "    target_paper_litreview = intros[i]\n",
        "    data.append({\"prompt\": instruction_prompt, \"completion\": completion})\n",
        "\n",
        "\n",
        "# Step 2: Save dataset to a JSONL file (required format for fine-tuning)\n",
        "fine_tune_file = \"./fine_tune_data.jsonl\"\n",
        "with open(fine_tune_file, 'w') as f:\n",
        "    for item in data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(f\"Fine-tuning dataset saved to {fine_tune_file}\")"
      ],
      "metadata": {
        "id": "ei6TKpKyOfd_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}